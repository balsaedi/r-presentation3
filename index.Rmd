---
title: "Statistical Programming, Simulation and Modeling"
author: "Dr. Basim Alsaedi"
date: "`r Sys.Date()`"
output:
  slidy_presentation:
    incremental: false
  ioslides_presentation:
    widescreen: true
    incremental: false
    logo: null
    css: custom.css
  beamer_presentation:
    incremental: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  comment = "##",
  warning = FALSE,
  message = FALSE,
  fig.width = 8,
  fig.height = 5,
  out.width = "100%",
  fig.align = "center"
)
```

# Introduction

## What is Statistical Programming?

Statistical programming combines:

- Programming skills
- Statistical knowledge

To perform tasks like:

- Data cleaning and preparation
- Exploration and visualization
- Statistical modeling
- Simulation and hypothesis testing
- Reproducible analyses

## Why Learn Statistical Programming?

- **Data literacy**: Understand and communicate with data
- **Problem-solving**: Apply statistics to real-world problems
- **Career opportunities**: Skills for data science and analytics
- **Research capabilities**: Design and conduct statistical analyses

# Probability Distributions and Simulation

## Why Simulate Data?

Simulation helps us:

- Understand theoretical distribution properties
- Test hypotheses when analytical solutions are difficult
- Generate synthetic data for testing
- Model real-world processes

## Normal Distribution

The normal distribution has the familiar bell shape:

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{- \frac{(x - \mu)^{2}}{2\sigma^{2}}}$$

```{r, echo=FALSE}
set.seed(42)
normal_data <- rnorm(1000, mean = 0, sd = 1)
hist(normal_data, breaks = 30, col = "skyblue", border = "white",
     main = "Normal Distribution",
     xlab = "Value", prob = TRUE)
curve(dnorm(x, mean = 0, sd = 1), add = TRUE, col = "red", lwd = 2)
```

## Simulating Normal Data in R

```{r, eval=FALSE}
# Generate random values from a normal distribution
normal_data <- rnorm(1000, mean = 0, sd = 1)

# Plot a histogram
hist(normal_data, breaks = 30, col = "skyblue", border = "white",
     main = "Normal Distribution",
     xlab = "Value", prob = TRUE)

# Add theoretical density curve
curve(dnorm(x, mean = 0, sd = 1), add = TRUE, col = "red", lwd = 2)
```

## Normal Distribution: Real-world Example

**Human Heights**:

```{r, echo=FALSE}
set.seed(123)
male_heights <- rnorm(500, mean = 175, sd = 7)
hist(male_heights, breaks = 20, col = "lightgreen", border = "white",
     main = "Distribution of Adult Male Heights",
     xlab = "Height (cm)", prob = TRUE)
curve(dnorm(x, mean = 175, sd = 7), add = TRUE, col = "darkgreen", lwd = 2)
```

## Binomial Distribution

Models the number of successes in fixed trials:

$$P(X = k) = \binom{n}{k}p^{k}(1 - p)^{n - k}$$

```{r, echo=FALSE}
set.seed(42)
binomial_data <- rbinom(1000, size = 10, prob = 0.3)
barplot(table(binomial_data)/1000, col = "salmon", 
        main = "Binomial Distribution (n = 10, p = 0.3)",
        xlab = "Number of Successes", ylab = "Probability")
points(0:10, dbinom(0:10, size = 10, prob = 0.3), pch = 16, col = "blue")
lines(0:10, dbinom(0:10, size = 10, prob = 0.3), col = "blue", lwd = 2)
```

## Simulating Binomial Data in R

```{r, eval=FALSE}
# Generate binomial random values
binomial_data <- rbinom(1000, size = 10, prob = 0.3)

# Plot the distribution
barplot(table(binomial_data)/1000, col = "salmon", 
        main = "Binomial Distribution (n = 10, p = 0.3)",
        xlab = "Number of Successes", ylab = "Probability")

# Add theoretical probabilities
points(0:10, dbinom(0:10, size = 10, prob = 0.3), 
       pch = 16, col = "blue")
lines(0:10, dbinom(0:10, size = 10, prob = 0.3), 
      col = "blue", lwd = 2)
```

## Binomial Distribution: Real-world Example

**Coin Flips**:

```{r, echo=FALSE}
set.seed(123)
coin_flips <- rbinom(1000, size = 20, prob = 0.5)
hist(coin_flips, breaks = seq(-0.5, 20.5, by = 1), col = "lightgreen",
     main = "Number of Heads in 20 Coin Flips",
     xlab = "Number of Heads")
```

## Poisson Distribution

Models events in fixed time/space intervals:

$$P(X = k) = \frac{\lambda^{k}e^{- \lambda}}{k!}$$

```{r, echo=FALSE}
set.seed(42)
poisson_data <- rpois(1000, lambda = 3)
barplot(table(poisson_data)/1000, col = "lightblue", 
        main = "Poisson Distribution (λ = 3)",
        xlab = "Number of Events", ylab = "Probability")
points(0:max(poisson_data), dpois(0:max(poisson_data), lambda = 3), 
       pch = 16, col = "red")
lines(0:max(poisson_data), dpois(0:max(poisson_data), lambda = 3), 
      col = "red", lwd = 2)
```

## Simulating Poisson Data in R

```{r, eval=FALSE}
# Generate Poisson random values
poisson_data <- rpois(1000, lambda = 3)

# Plot the distribution
barplot(table(poisson_data)/1000, col = "lightblue", 
        main = "Poisson Distribution (λ = 3)",
        xlab = "Number of Events", ylab = "Probability")

# Add theoretical probabilities
points(0:max(poisson_data), dpois(0:max(poisson_data), lambda = 3), 
       pch = 16, col = "red")
lines(0:max(poisson_data), dpois(0:max(poisson_data), lambda = 3), 
      col = "red", lwd = 2)
```

## Poisson Distribution: Real-world Example

**Call Center Arrivals**:

```{r, echo=FALSE}
set.seed(123)
hourly_calls <- rpois(24, lambda = 15)
barplot(hourly_calls, names.arg = 1:24, col = "skyblue",
        main = "Simulated Hourly Call Arrivals",
        xlab = "Hour of Day", ylab = "Number of Calls")
abline(h = 15, col = "red", lwd = 2, lty = 2)
```

## Uniform Distribution

Equal probability across all values in a range:

$$f(x) = \frac{1}{b - a}, \quad a \leq x \leq b$$

```{r, echo=FALSE}
set.seed(42)
uniform_data <- runif(1000, min = 0, max = 1)
hist(uniform_data, breaks = 20, col = "lightgoldenrod", border = "white",
     main = "Uniform Distribution (min = 0, max = 1)",
     xlab = "Value", prob = TRUE)
abline(h = 1, col = "red", lwd = 2)
```

## Simulating Uniform Data in R

```{r, eval=FALSE}
# Generate uniform random values
uniform_data <- runif(1000, min = 0, max = 1)

# Plot a histogram
hist(uniform_data, breaks = 20, col = "lightgoldenrod", border = "white",
     main = "Uniform Distribution (min = 0, max = 1)",
     xlab = "Value", prob = TRUE)

# Add the theoretical density line
abline(h = 1, col = "red", lwd = 2)
```

## Exponential Distribution

Models time between events:

$$f(x) = \lambda e^{-\lambda x}, \quad x \geq 0$$

```{r, echo=FALSE}
set.seed(42)
exponential_data <- rexp(1000, rate = 0.5)
hist(exponential_data, breaks = 30, col = "thistle", border = "white",
     main = "Exponential Distribution (rate = 0.5)",
     xlab = "Value", prob = TRUE)
curve(dexp(x, rate = 0.5), from = 0, to = 10, add = TRUE, 
      col = "purple", lwd = 2)
```

## Simulating Exponential Data in R

```{r, eval=FALSE}
# Generate exponential random values
exponential_data <- rexp(1000, rate = 0.5)

# Plot a histogram
hist(exponential_data, breaks = 30, col = "thistle", border = "white",
     main = "Exponential Distribution (rate = 0.5)",
     xlab = "Value", prob = TRUE)

# Add the theoretical density curve
curve(dexp(x, rate = 0.5), from = 0, to = 10, add = TRUE, 
      col = "purple", lwd = 2)
```

## Exponential Distribution: Real-world Example

**Customer Service Times**:

```{r, echo=FALSE}
set.seed(123)
service_times <- rexp(200, rate = 1/3)
hist(service_times, breaks = 20, col = "lightblue", border = "white",
     main = "Distribution of Customer Service Times",
     xlab = "Service Time (minutes)", prob = TRUE)
curve(dexp(x, rate = 1/3), from = 0, to = 15, add = TRUE, 
      col = "blue", lwd = 2)
```

# Monte Carlo Simulation

## What is Monte Carlo Simulation?

Monte Carlo methods use repeated random sampling to:

1. Define the domain of possible inputs
2. Generate inputs randomly from the domain
3. Perform computations using these inputs
4. Aggregate the results

## Applications of Monte Carlo Simulation

- Statistics and probability calculations
- Finance for risk assessment
- Physics and chemistry simulations
- Engineering for reliability analysis
- Machine learning for uncertainty estimation

## Estimating Probabilities: Example 1

**Probability of rolling a sum of 7 with two dice**:

```{r, echo=FALSE}
set.seed(42)
n_simulations <- 10000

roll_two_dice <- function() {
  sum(sample(1:6, 2, replace = TRUE))
}

rolls <- replicate(n_simulations, roll_two_dice())
prob_sum_7 <- mean(rolls == 7)

barplot(table(rolls)/n_simulations, col = "skyblue",
        main = paste("Distribution of Dice Sums\nP(Sum = 7) =", round(prob_sum_7, 4)),
        xlab = "Sum of Two Dice", ylab = "Probability")
abline(h = 1/6, col = "red", lwd = 2, lty = 2)
```

## Monte Carlo for Probability: Code

```{r, eval=FALSE}
# Function to simulate rolling two dice
roll_two_dice <- function() {
  sum(sample(1:6, 2, replace = TRUE))
}

# Run many simulations
rolls <- replicate(10000, roll_two_dice())

# Estimate the probability
prob_sum_7 <- mean(rolls == 7)
cat("Estimated probability:", prob_sum_7, "\n")
cat("Theoretical probability:", 6/36, "\n")
```

## Estimating π Using Monte Carlo

Using random points in a square to estimate π:

```{r, echo=FALSE}
set.seed(42)
n <- 2000

x <- runif(n, min = -1, max = 1)
y <- runif(n, min = -1, max = 1)
inside_circle <- (x^2 + y^2) <= 1
pi_estimate <- 4 * mean(inside_circle)

plot(x, y, pch = 19, cex = 0.5, 
     col = ifelse(inside_circle, "blue", "red"),
     main = paste("Monte Carlo Estimation of π =", round(pi_estimate, 6)),
     xlab = "x", ylab = "y", asp = 1)
symbols(0, 0, circles = 1, inches = FALSE, add = TRUE)
```

## Monte Carlo Integration

Numerical integration using random numbers:

```{r, echo=FALSE}
set.seed(42)
n <- 1000

x <- runif(n, min = 0, max = 1)
y <- x^2
integral_estimate <- mean(y)

plot(x[1:200], y[1:200], pch = 19, cex = 0.8, col = "blue",
     main = expression(paste("Monte Carlo Integration of ", integral(x^2, 0, 1))),
     xlab = "x", ylab = expression(x^2))
curve(x^2, from = 0, to = 1, add = TRUE, col = "red", lwd = 2)
text(0.7, 0.1, paste("Estimate =", round(integral_estimate, 4)))
text(0.7, 0.05, "True value = 1/3")
```

## Estimating Expected Values

Monte Carlo for expected values of complex distributions:

```{r, echo=FALSE}
set.seed(42)
n <- 100000

x <- rnorm(n, mean = 0, sd = 1)
exp_x <- exp(x)
expected_value <- mean(exp_x)
theoretical_value <- exp(0.5)

hist(exp_x, breaks = 50, prob = TRUE, col = "lightgreen",
     main = paste("E[e^X] where X ~ N(0,1)\nEstimate =", round(expected_value, 4)),
     xlab = "e^X", ylim = c(0, 0.8), xlim = c(0, 10))
```

## Portfolio Returns Simulation

```{r, echo=FALSE}
set.seed(42)
n_simulations <- 10000

assets <- data.frame(
  name = c("Stocks", "Bonds", "Cash"),
  expected_return = c(0.08, 0.04, 0.01),
  risk = c(0.18, 0.08, 0.01)
)

weights <- c(0.6, 0.35, 0.05)

simulate_portfolio <- function() {
  asset_returns <- numeric(nrow(assets))
  
  for (i in 1:nrow(assets)) {
    asset_returns[i] <- rnorm(1, 
                            mean = assets$expected_return[i], 
                            sd = assets$risk[i])
  }
  
  return(sum(weights * asset_returns))
}

portfolio_returns <- replicate(n_simulations, simulate_portfolio())
expected_return <- mean(portfolio_returns)
portfolio_risk <- sd(portfolio_returns)
prob_negative <- mean(portfolio_returns < 0) * 100

hist(portfolio_returns, breaks = 50, col = "skyblue", border = "white",
     main = "Portfolio Returns Simulation",
     xlab = "Annual Return", prob = TRUE)
abline(v = expected_return, col = "red", lwd = 2)
abline(v = 0, col = "darkred", lty = 2, lwd = 2)
```

## Portfolio Returns: Key Statistics

For a portfolio with 60% stocks, 35% bonds, and 5% cash:

```{r, echo=FALSE}
cat("Expected portfolio return:", round(expected_return * 100, 2), "%\n")
cat("Portfolio risk (standard deviation):", round(portfolio_risk * 100, 2), "%\n")
cat("Probability of negative return:", round(prob_negative, 2), "%\n")
```

# Advanced Topics

## Central Limit Theorem

As sample size increases, the distribution of sample means approaches a normal distribution:

```{r, echo=FALSE, fig.height=4}
set.seed(42)
n_simulations <- 5000

# Function for a single row of histograms
plot_sample_means <- function(dist_func, dist_name, sample_sizes=c(1, 5, 30)) {
  par(mfrow=c(1, 3), mar=c(4,4,3,1))
  
  for (size in sample_sizes) {
    sample_means <- replicate(n_simulations, mean(dist_func(size)))
    hist(sample_means, breaks=30, col="lightblue", border="white",
         main=paste(dist_name, "- n =", size),
         xlab="Sample Mean", freq=FALSE)
    curve(dnorm(x, mean=mean(sample_means), sd=sd(sample_means)), 
          add=TRUE, col="red", lwd=2)
  }
}

# Exponential distribution demonstration
plot_sample_means(
  function(n) rexp(n, rate=1),
  "Exponential Distribution"
)
```

## Effects of Outliers on Regression

```{r, echo=FALSE, fig.height=4}
set.seed(42)
n <- 50

# Generate clean data
x <- runif(n, 0, 10)
y_clean <- 2 + 3 * x + rnorm(n, 0, 2)

# Add an outlier
x_outlier <- c(x, 9.5)
y_outlier <- c(y_clean, 50)

# Fit models
model_clean <- lm(y_clean ~ x)
model_outlier <- lm(y_outlier ~ x_outlier)

par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

# Clean data
plot(x, y_clean, pch = 16, col = "blue",
     main = "Clean Data",
     xlab = "x", ylab = "y")
abline(model_clean, col = "red", lwd = 2)

# Outlier data
plot(x_outlier, y_outlier, pch = 16, 
     col = ifelse(1:length(x_outlier) <= n, "blue", "red"),
     main = "Data with Outlier",
     xlab = "x", ylab = "y")
abline(model_outlier, col = "red", lwd = 2)
```

## Regression Coefficients Comparison

```{r, echo=FALSE}
cat("Clean data model: y =", round(coef(model_clean)[1], 2), "+", 
    round(coef(model_clean)[2], 2), "x\n")
cat("Outlier data model: y =", round(coef(model_outlier)[1], 2), "+", 
    round(coef(model_outlier)[2], 2), "x\n")
```

A single outlier dramatically changed the regression line!

## Bootstrap Resampling

Estimating sampling distributions by resampling the original data:

```{r, echo=FALSE}
set.seed(42)
n_bootstrap <- 5000

# Original data (e.g., exam scores)
original_data <- c(72, 85, 93, 65, 70, 79, 88, 92, 81, 76, 85, 90)
sample_mean <- mean(original_data)
bootstrap_means <- replicate(n_bootstrap, mean(sample(original_data, replace = TRUE)))
bootstrap_ci <- quantile(bootstrap_means, c(0.025, 0.975))

hist(bootstrap_means, breaks = 30, col = "skyblue", border = "white",
     main = "Bootstrap Distribution of the Mean",
     xlab = "Sample Mean")
abline(v = sample_mean, col = "red", lwd = 2)
abline(v = bootstrap_ci, col = "blue", lwd = 2, lty = 2)
```

## Bootstrap: Key Results

For our original sample:

```{r, echo=FALSE}
cat("Original sample mean:", sample_mean, "\n")
cat("95% bootstrap confidence interval:", bootstrap_ci[1], "to", bootstrap_ci[2], "\n")
```

## Practice Exercise: Basic Probability

**Problem**: What's the probability of at least one 6 in three dice rolls?

```{r, echo=FALSE}
set.seed(123)
n_simulations <- 100000

simulate_dice <- function() {
  rolls <- sample(1:6, 3, replace = TRUE)
  return(any(rolls == 6))
}

results <- replicate(n_simulations, simulate_dice())
prob_at_least_one_6 <- mean(results)
theoretical_prob <- 1 - (5/6)^3

barplot(c(prob_at_least_one_6, theoretical_prob), 
        names.arg = c("Simulation", "Theory"),
        col = c("lightblue", "lightgreen"),
        main = "Probability of ≥1 Six in Three Dice Rolls",
        ylim = c(0, 0.5))
text(1, prob_at_least_one_6 + 0.03, round(prob_at_least_one_6, 3))
text(2, theoretical_prob + 0.03, round(theoretical_prob, 3))
```

## Practice Exercise: Confidence Intervals

How sample size affects confidence interval width:

```{r, echo=FALSE, fig.height=4}
set.seed(789)
n_simulations <- 1000
population_mean <- 50
population_sd <- 10
sample_sizes <- c(5, 10, 30, 100, 500)

create_ci <- function(sample, conf_level = 0.95) {
  n <- length(sample)
  mean_sample <- mean(sample)
  se <- sd(sample) / sqrt(n)
  t_critical <- qt(1 - (1 - conf_level)/2, df = n - 1)
  
  ci_lower <- mean_sample - t_critical * se
  ci_upper <- mean_sample + t_critical * se
  
  return(c(ci_lower, ci_upper))
}

results <- list()

for (size in sample_sizes) {
  ci_widths <- numeric(n_simulations)
  contains_true_mean <- logical(n_simulations)
  
  for (i in 1:n_simulations) {
    sample_data <- rnorm(size, mean = population_mean, sd = population_sd)
    ci <- create_ci(sample_data)
    ci_widths[i] <- ci[2] - ci[1]
    contains_true_mean[i] <- (ci[1] <= population_mean) && (ci[2] >= population_mean)
  }
  
  results[[as.character(size)]] <- list(
    widths = ci_widths,
    coverage = mean(contains_true_mean)
  )
}

par(mfrow = c(1, 1))
boxplot(lapply(results, function(x) x$widths), 
        names = sample_sizes,
        main = "95% CI Widths by Sample Size",
        xlab = "Sample Size", ylab = "CI Width",
        col = "lightblue")
```

# Conclusion

## Key Takeaways

- **Statistical Programming** combines programming and statistics for data analysis
- **Probability Distributions** model different types of random phenomena
- **Simulation Techniques** allow us to:
  - Generate synthetic data for testing
  - Estimate complex probabilities
  - Calculate integrals and expected values
  - Verify statistical theories
  - Assess statistical method robustness

## Further Reading

1. Ross, S. M. (2013). Simulation. Academic Press.
2. Robert, C. P., & Casella, G. (2010). Monte Carlo Statistical Methods. Springer.
3. Rizzo, M. L. (2019). Statistical Computing with R. CRC Press.
4. Wickham, H., & Grolemund, G. (2017). R for Data Science. O'Reilly Media.

## Thank You!

Questions?

```{r, echo=FALSE, out.width="50%"}
# Generate a word cloud of key terms
library(wordcloud)
set.seed(42)
terms <- c(rep("Simulation", 10), rep("Monte Carlo", 8), rep("Probability", 8), 
           rep("Distribution", 7), rep("Normal", 5), rep("Binomial", 5),
           rep("Poisson", 5), rep("Exponential", 5), rep("Statistics", 7),
           rep("R", 6), rep("Random", 6), rep("Data", 5), rep("Sampling", 4),
           rep("Bootstrap", 4), rep("Integration", 3), rep("Visualization", 3),
           rep("Modeling", 4), rep("Programming", 5), rep("Analysis", 4))
wordcloud(terms, min.freq = 1, colors = brewer.pal(8, "Dark2"),
          random.order = FALSE)
```
